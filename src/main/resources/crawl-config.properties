#crawler configuration properties

#Crawler will spawn <noOfCrawlers> threads for performing crawling  
numberOfCrawlers=10

#this is used for crawler internal purpose and should not be changed unless really required
crawlStorageFolder=/etc/site-404-checker/crawl/root

#During crawling a particular start URL crawling will go upto <maxDepthOfCrawling>
maxDepthOfCrawling=10

#Once started crawling will fetch only <maxPagesToFetch>. after that it will stop
maxPagesToFetch=1000

#During Crawling, Crawler will hit two urls with internal of <politeness Delay>
politenessDelay=200

#This string is used by Crawler for identifing itself to robots.txt file
userAgentString=crawler4j (http://code.google.com/p/crawler4j/)

#If you want to run Crawler behind any proxy use below properties
#proxyHost=proxyserver.example.com
#proxyPort=8080
#proxyUser=username
#proxyPassword=password


#Add the URL which you don't want to crawl in the below list of URLS
#excludeURL=www.example.com/blog,www.example-two.com/test


#Add the extensions that you don't want to crawl in the below list of extensions
excludeExtension=css,js,gif,jpgpng,mp3,mp4,zip,gz,html

